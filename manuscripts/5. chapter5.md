# 第五章—網絡存取

在這章中，我們使用 Python 來存取網絡資源。

首先，我們使用 WebBrowser 來打開網址。

然後，我們使用 Requests 來下載網絡資源，再使用 BeautifulSoup 來將下載回來的 HTML 分析成為 HTML DOM 樹。

最後，我們使用 Selenium 來使用控制瀏覽器來對網頁進行互動。

## 使用 WebBrowser 打開網址

webbrowser 是內置模組，我們不用額外安裝，但使用前，需要利用 `import` 來載入此模組。

```
import webbrowser

query = input("Please input search query to search Python doc. ")

url = "https://docs.python.org/3/search.html?q=" + query + "&check_keywords=yes&area=default"

webbrowser.open(url)
```

在這個例子中，我們首先利用 input 要求使用者輸入一個查詢字串。然後
WebBrowser 例子二

舉一反三

```
import webbrowser

query = input("Please input search query to search near-by Macao: ")
```

# A map search in Macao.

```
url = "https://www.google.com/maps/search/{}/@22.1612464,113.5303786,13z".format(query)

webbrowser.open(url)
```

我平常使用 iPad，會有一些文件可以使用 x-callback-url 來直接打開文件，這時，利用 iOS 上的 Pythonista 執行 `webbrowser.open` 來設定打開文件的代碼。

```
import webbrowser
webbrowser.open('x-devonthink-item://E3EEB992-F06C-4B0E-9E04-20C3F5798994')
```

## 使用 Requests 下載 HTML 文件

首先，我們要安裝 `requests` 及 `BeautifulSoup` 兩個模組。
如果未安裝，可以使用 `pip install requests` 及 `pip install BeautifulSoup` 來安裝。

```
from bs4 import BeautifulSoup
import requests

try:
    res = requests.get("https://news.gov.mo/home/zh-hant")
except requests.exceptions.ConnectionError:
    print("Error: Invalid URL")
    exit()


soup = BeautifulSoup(res.text, "lxml")

print("\n".join( map(lambda x: x.getText().strip(), soup.select("h5"))) )
```

## 使用 CSS Selector 選擇需要的目標 HTML 元素及當中的內容

當我們取得整個網頁 HTML 檔案後，可以利用 BeautifulSoup 及 lxml 將 HTML 分析成為 DOM 結構樹。這個結構樹下，我們可以通過 CSS Selector 的寫法，來選取各種條件下的 HTML 元素，並且可以繼續搜尋所選取元素的前後、上層及下層元素，直至找到我們要找的內容為止。

找到元素 element 後，如果需要其文字內容，可以通過 `element.getText()` 或 element.text 來取得其文字內容。
而如果是 `<a>` 連結，通常我們是需要取得其連結 URL 位置，以便進一步取得當中的內容，或用以直接下載成檔案。則我們可以通過 element['href'] 來讀取元素 href 屬性。

## 一些常用的 CSS Selector

選取所有 `<a>` 元素

`a`

選取所有 class='news-title' 元素

`.news-title`

選取 id='main-title' 元素

`#main-title`

選取 `<main>` 內的所有 `<p>` 元素

main p

選取 href 結尾為 .pdf 的所有 `<a>` 元素

`a[href$=".pdf"]`

選取 `<article>` 內的第一個 `<p>` 元素（注：很多時候 article 內第一個元素是標題，然後才到 `<p>` 元素的文字內容或其他 `<div>`）

article p:first-of-type

選取 `<nav>` 下一層的 `<ul>` 的下一層的所有 `<li>` 元素


`nav > ul > li`

例子：讀取 news.gov.mo 首頁的新聞標題及每篇的第一段文字
以下例子，我們便會在讀取每個新聞標題文字後，再讀取當中每一篇內容的第一段文字。然後以純文字格式將結果列印出來。

```
from bs4 import BeautifulSoup
import requests

url_base = 'https://news.gov.mo/home/zh-hant/'

try:
    res = requests.get(url_base)
except requests.exceptions.ConnectionError:
    print("Error: Invalid URL")
    exit()


soup = BeautifulSoup(res.text, "lxml")

for news in soup.select("h5"):
    href = news.select('a')[0]['href']
    print(news.getText().strip() + '\n')
    # print(url_base + href)
    # fetch news content
    res = requests.get(url_base + href)
    s = BeautifulSoup(res.text, "lxml")
    content = s.select(".asideBody .langInfo p")
    print(content[2].getText() + '\n') # first 2 p are empty
    print('----\n')
```

## 存取 XML 資訊
針對 HTML 網頁文件，我們使用了 requests 及 BeautifulSoup 兩個模組，而如果我們需要存取 XML 文件，除了 requests 模組下載外，亦可以直接使用 untangle 來將網絡上的 XML 加以分析及獲取當中的內容。

```
import untangle
import datetime

def fetch():
    '''Fetch the xml.SMG.gov.mo to obtain current temperature data of Macao.'''
    obj = untangle.parse('http://xml.smg.gov.mo/c_actual_brief.xml')

    temperature = obj.ActualWeatherBrief.Custom.Temperature.Value.cdata
    humidity = obj.ActualWeatherBrief.Custom.Humidity.Value.cdata

    print("現時澳門氣溫 " + temperature + " 度，濕度 " + humidity + "%。")

fetch()
```

## 使用 Session 進行登入操作
TODO

## 儲存取得的資訊
下載檔案
我們可以使用 urlretrieve 來下載檔案，使用方法是提供網址，然後我們將網址最後一節視為檔案名稱。

```
from urllib.request import urlretrieve

url = 'https://makzan.net/resume.pdf'
filename = url.split('/')[-1]
urlretrieve(url, filename)
print("Downloaded file: {}".format(filename))
```


## 儲存數據
我們可以將下載回來的數據加以儲存。久而久之，當我們儲存足夠多數據，便可以用來做數據分析。




## 使用 Selenium 及 WebDriver

## 安裝 Selenium

## 安裝瀏覽器 Driver
我們需要選擇一個瀏覽器引擎作為 WebDriver。Selenium 只是代我們操作瀏覽器，若沒有瀏覽器亦然沒有用武之地。
主流瀏覽器 WebDriver 有 Chrome、Firefox、Edge、Safari。我通常使用 Firefox。以下我們利用 Firefox 作為例子。

## Windows

## Mac

如果有使用 homebrew(brew.sh)，可以直接執行 `brew install` 來安裝。

```
$ brew install geckodriver
```

否則，則需要自行在其開源 GitHub Repo 下載 geckodriver 運行檔，在放到執行 PATH 上。

## 儲存網站截圖截圖

```
'''Capture the screenshot of a website via Firefox.'''

from selenium import webdriver

driver = webdriver.Firefox()
driver.maximize_window()
driver.get('https://makclass.com/')
driver.save_screenshot('test2.png')
driver.close()
```


運行時，會見到系統打開 Firefox 然後載入我們指定的網頁。

但細想，如果每次都打開瀏覽器，豈不是很麻煩很阻礙眼？所以我們可以將瀏覽器以 Headless 模式來運行。Headless 意思是不會開啟 GUI 界面，非常適合於 Command Line 的純文字模式下使用。

Firefox 的技術說明文件就有介紹 Headless 模式：
https://developer.mozilla.org/en-US/docs/Mozilla/Firefox/Headless_mode

以下為修改後的截圖代碼。

```
'''Capture the screenshot of a website via Headless Firefox.'''

from selenium import webdriver
from selenium.webdriver.firefox.options import Options

options = Options()
options.add_argument('-headless')

driver = webdriver.Firefox(options=options)
driver.maximize_window()
driver.get('https://makclass.com/test_projects')
driver.save_screenshot('test2.png')
driver.close()
```

但其實，如果要截圖，使用 Firefox 有更簡單方法，直接瀏覽器功能可以在 Command Line 輸出網站截圖，而且是。

```
$ /Applications/Firefox.app/Contents/MacOS/firefox -screenshot makzan-net.png https://makzan.net
```




取得網站截圖數據

## Pillow 圖像修改

Pillow 是一個第三方模組，所以我們使用之前需要使用 pip 來安裝。

Pillow 的前身是 PIL，在 Python2 的年代非常盛行，可惜後來 PIL 逐漸日久失修，沒有了當初的維護質素，而 Pillow 則是從 PIL 出來的 Python3 圖像修改模組。
要安裝 Pillow，可以使用 `pip install`

```
$ pip install pillow
```

todo

儲存整頁截圖

## Selenium 自動化操作


Selenium 有幾種找尋元素的方法：

- find_element_by_id
- find_element_by_name
- find_element_by_xpath
- find_element_by_link_text
- find_element_by_partial_link_text
- find_element_by_tag_name
- find_element_by_class_name
- find_element_by_css_selector

其中，find_element_by_css_selector 是比較通用的，因為可以通過 CSS Selector 很彈性地選擇目標所需要的 HTML 元素。